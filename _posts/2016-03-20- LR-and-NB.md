---
published: true
author: Charles
layout: post
title:  "Logistic Regression and Naive Bayes"
date:   2016-03-20 9:30
categories: 机器学习
---

#### 数学表示
我们先推导下 Logistic Regression 和 Naive Bayes在数学表达上的相关性[^1].

$$\begin{align*}
P(Y=1|X) & = \frac{P(Y=1)P(X|Y=1)}{P(Y=1)P(X|Y=1)+P(Y=0)P(X|Y=0)}\\
& = \frac{1}{1+\cfrac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)}}\\
& = \frac{1}{1+\exp(\ln\frac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)})}\\
& = \frac{1}{1+\exp((\ln \cfrac{1-\pi}{\pi})+\sum_i\ln\cfrac{P(X_i|Y=0)}{P(X_i|Y=1)})} \tag{1}
\end{align*}$$


----------


我们假设 $P(X_i\|Y=y_k)$ 服从高斯分布（Gaussian Naive Bayes）： $\mathcal{N}(\mu_{ik},\delta_i)$，

$$\begin{align*}
\sum_i\ln\frac{P(X_i|Y=0)}{P(X_i|Y=1)} & = \sum_i\ln\frac{\frac{1}{\sqrt{2\pi\delta_i^2}}\exp(\frac{-(X_i-\mu_{i0})^2}{2\delta_i^2})}{\frac{1}{\sqrt{2\pi\delta_i^2}}\exp(\frac{-(X_i-\mu_{i1})^2}{2\delta_i^2})}\\
& = \sum_i \left( \frac{(X_i-\mu_{i1})^2-(X_i-\mu_{i0})^2}{2\delta_i^2} \right)\\
& = \sum_i \left( \frac{2X_i(\mu_{i0}-\mu_{i1})+\mu_{i1}^2-\mu_{i0}^2}{2\delta_i^2} \right)\\
& = \sum_i \left( \frac{\mu_{i0}-\mu_{i1}}{\delta_i^2} X_i + \frac{\mu_{i1}^2-\mu_{i0}^2}{\delta_i^2} \right) \tag{2}
\end{align*}$$


----------


从而，
$$\begin{align*}
\Rightarrow P(Y=1|X) & = \frac{1}{1+\exp((\ln \frac{1-\pi}{\pi})+\sum_i ( \frac{\mu_{i0}-\mu_{i1}}{\delta_i^2} X_i + \frac{\mu_{i1}^2-\mu_{i0}^2}{\delta_i^2} ))}\\
& = \frac{1}{1+\exp(w_0+\sum_{i=1}^{n}w_iX_i)}  \tag{3}
\end{align*}$$

其中，

$$\begin{align*}
w_0 & = \ln \frac{1-\pi}{\pi} + \sum_i \frac{\mu_{i1}^2-\mu_{i0}^2}{\delta_i^2}\\
w_i & = \frac{\mu_{i0}-\mu_{i1}}{\delta_i^2}
\end{align*}$$


----------

#### 区别[^2]

 -  Naive Bayes是一个**生成模型**，在计算 $P(y\|x)$ 之前，先要从训练数据中计算 $P(x\|y)$ 和 $P(y)$ 的概率，从而利用贝叶斯公式计算 $P(y\|x)$ 。 Logistic Regression 是一个**判别模型**，它通过在训练数据集上最大化判别函数 $P(y\|x)$ 学习得到，不需要知道 $P(x\|y)$ 和 $P(y)$。
 -  Naive Bayes是建立在**条件独立**假设的基础上， Logistic Regression的限制则要宽松很多，如果数据满徐条件独立假设，Logistic Regression能够取得很好的效果；当数据不满度条件独立假设时，Logistic Regression仍然能够通过调整参数让模型最大化的符合数据的分布，从而训练得到在现有数据集下的一个最优模型。
 -  Naive Bayes运用了比较严格的条件独立假设，为了计算 $P(y\|x)$，我们可以利用统计的方法统计数据集中 $P(x\|y)$ 和 $P(y)$ 出现的次数，从而求得 $P(x\|y)$ 和 $P(y)$。因而其所需的数据量要小一些，为 $\mathcal{O}(\log n)$. Logistic Regression 在计算时，是在整个参数空间进行线性搜索的，需要的数据集就更大，为$\mathcal{O}(n)$

![此处输入图片的描述][1]

----------

[1]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-29_211257.png

[^1]: [GENERATIVE AND DISCRIMINATIVE CLASSIFIERS: NAIVE BAYES AND LOGISTIC REGRESSION](http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)
[^2]: [朴素贝叶斯 VS 逻辑回归 区别](http://blog.csdn.net/chlele0105/article/details/38922551)