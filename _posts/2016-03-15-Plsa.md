---
published: true
author: Charles
layout: post
title:  "Probabilistic Latent Semantic Analysis"
date:   2016-03-15 15:30
categories: 自然语言处理 推荐系统
---

之前谈过的 "[Latent Semantic Analysis][1]"，可以用来解决一义多词的问题，但缺乏严谨的数理统计基础，同时SVD分解非常耗时，pLSA就是在这样的背景下被提出的。

![此处输入图片的描述][2]


----------

#### pLSA
和很多模型一样，pLSA 遵从 bag-of-words 假设，即只考虑一篇文档中单词出现的次数，而忽略单词的先后次序关系，且每个单词的出现都是彼此独立的。

我们来看下pLSA的核心思想，

![此处输入图片的描述][3]

实心的节点 d 和 w 表示我们能观察到的文档和单词，空心节点 z 表示我们观察不到的隐藏变量，用来表示隐含的主题。

pLSA假设每篇文档的生成过程是这样的：

- 以 $P(d)$ 的先验概率选择一篇文档 $d$      
- 选定 d 后，以 $P(z\|d)$ 的概率选中主题 z       
- 选中主题 z 后，以 $P(w\|z)$ 的概率选中单词 w       

> 并且每个主题在所有词项上服从 Multinomial 分布，每个文档在所有主题上服从 Multinomial 分布。

![此处输入图片的描述][4]

同时我们认为在给定 Topic z 的条件下，单词 w 和文档 d 之间是条件独立的，也就说：

$$\begin{align*}
P(w|d,z) & = P(w|z)\\
P(w,d|z) & = P(w|z)P(d|z)
\end{align*}$$

那么，

$$\begin{align*}
P(w,d) &= P(d)P(w|d)  \tag{1}\\
P(w|d) & = \sum_{z\in \mathcal{Z}} P(w,z|d)\\
&= \sum_{z\in \mathcal{Z}} P(w|d)P(z|d) \tag{2}
\end{align*}$$

再来一发神图，

![此处输入图片的描述][5]


----------


####  Estimate parameters
模型中的参数包括： $P(z|d)$  和 P(w|z)。我们可以采用极大似然估计，目标函数为：

$$L = \prod_{(d,w)} P(w,d) = \prod_{d\in \mathcal{D}}\prod_{w\in \mathcal{W}} P(w,d)^{n(d,w)} \tag{3}$$

其中 $n(d,w)$ 表示单词 w 在文档 d 中出现的次数。

取对数，

$$\begin{align*}
\arg\max_\theta L(\theta)
& = \arg\max_\theta \sum_{d,w} n(d,w)\log P(d,w;\theta)\\
&=  \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta)P(d) \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta) + \underbrace{\sum_{d,w} n(d,w)\log P(d)}_{\text{与} \theta \text{无关项}} \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log \sum_z P(w|z)P(z|d)
\end{align*}$$

这是一个非凸优化（non-convex optimization）问题，我们利用EM算法来估计参数（对于这种包含**隐藏变量**的参数估计，果断选EM算法呀）。

#### EM通俗版
----------


> 简版：猜（E-step）,反思（M-step）,重复；

啰嗦版：你知道一些东西（观察的到的数据），你不知道一些东西（隐藏变量），你很好奇，想知道点那些不了解的东西。怎么办呢？     
你根据一些假设（parameter）先猜（E-step），把那些不知道的东西都猜出来，假装你全都知道了;    然后有了这些猜出来的数据，你反思一下，更新一下你的假设（parameter）,   
让你观察到的数据更加可能(Maximize likelihood; M-step);    
然后再猜，再反思，最后，你就得到了一个可以解释整个数据的假设了。


----------


**E step:**

$$\begin{align*}
E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)]
&= \sum_z P(z|d,w;\theta_t) \log P(w, z|d;\theta) \\
&= \sum_z P(z|d,w;\theta_t) [\log P(w|z) + \log P(z|d)] \\
\end{align*}$$

----------
----------


  [1]: http://charlesx.top/2016/03/Latent-Semantic-Analysis/
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/f1_plsa.jpg
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_152408.png?imageView2/2/w/400
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/f2.jpg
  [5]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_161734.png?imageView2/2/w/400