---
published: true
author: Charles
layout: post
title:  "聚类算法总结"
date:   2016-06-20 14:28
categories: 机器学习
---

# 聚类算法总结

---

#### 去均值处理

(待整理)

### 聚类

[聚类算法总结](http://blog.chinaunix.net/uid-10289334-id-3758310.html)

#### 定义

> 聚类对大量**未标注**的数据集，按数据的**内在相似性**将数据集划分为多个类别，使得类别内的数据相似性尽可能大，而各类别间的数据相似性尽可能小。

#### 基本思想

> 基本思想：对于给定的k，算法首先给出一个初始的划分方法，之后通过反复迭代的方法改变划分，使得每一次改进之后的划分方案都较前一次更好。

#### 分类

聚类算法大致可以分为划分法（Partitioning Methods，K-means）、 层次法（Hierarchical Methods）、基于密度的方法（density-based methods）、 基于网格的方法（grid-based methods）、基于模型的方法（Model-Based Methods）。

#### 聚类结果评价

- 簇内不相似度
- 簇间不相似度
- 轮廓系数

#### K-means聚类

基本思想： K-means算法首先随机地选择个对象，每个对象初始地代表一个簇的平均值或中心。对剩余的每个对象根据其与各个簇中心的距离（之前的各种距离的定义，在这里派上了用场，可以根据实际需求来选择不同的距离定义），将它赋给最近的簇。然后重新计算每个簇的平均值。这个过程不断重复，直到准则函数函数收敛（准则函数常常使用最小均方误差）。

---

假定输入样本为 $S = x_1,x_2,\cdots,x_m$

- 选定初始的k个类别中心，$\mu_1,\mu_2,\cdots,\mu_k$
- 对于每个样本，将其标记为距离类别中心最近的类别，
$$\text{label}_i = \underset{1 \le j \le k}{\arg \min}||x_i-\mu_j||$$
- 将每个类别中心更新为隶属于该类别的所有样本的均值
$$\mu_j = \frac{1}{|c_j|}\sum_{i \in C_j} x_i$$
- 重复最后两步，直到类别中心的变化小于某阈值

![][1]

---

算法缺陷：

- 聚类中心的个数需要事先指定
- 初始聚类中心的选取，对初值敏感
- 不适合发现非凸形状的簇或者大小差别很大的簇
- K-means将簇中所有点的均值作为新质心，若簇中含有异常点，将导致均值偏离严重，可以采用K-mediods（K中值距离）

---

#### 层次聚类方法

层次性聚类方法对给定的数据集进行层次分解，直到某种条件满足为止，具体又可分为：

![][2]

- 凝聚的层次聚类：AGNES算法

> 一种**自底向上**的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。

- 分裂的层次聚类：DIANA算法

> 采用**自顶向下**的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。

---

### 密度聚类方法

密度聚类方法的基本思想是，只要一个区域中的点的密度大于某个域值，就把它加到与之相近的聚类中去。

这类算法能克服基于距离的算法只能发现“类圆形”的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。

#### DBSCAN

DBSCAN算法有两个重要的参数：Eps 和 MinPts。前者为定义密度时的邻域半径，后者为定义核心点时的阈值。

我们将点按照一定规则进行分类：核心点，边界点，噪声点。

![][3]

通俗地讲，核心点对应稠密区域内部的点，边界点对应稠密区域边缘的点，而噪声点对应稀疏区域中的点。

优点：

- 不需要事先指定cluster的数目
- 可以发现任意形状的cluster
- 能够找出数据中的噪声点，且对噪声不敏感

缺点：

- DBSCAN不适用于数据集密度差异很大的情形

---

#### OPTICS

其中 X 是原始的点的数据集合，P 作为最后输出结果的点集的有序序列，seedlist 是一个无重复元素的队列， 保存了当前已找到但是还没处理过的属于同一类别的点的集合。算法首先遍历输入的点集 X ，找到一个核心点，如果找不到，则直接结束算法，如果找到一个点 x 为核心点， 则设置 x 的核心距离以及可达距离并将其加入输出序列 P，将其所有邻点加入队列 seedlist。 之后从队列 seedlist 中找出可达距离最小的点 y，将其加入输出序列，如果 y 也是核心点， 则将 y 的所有邻点加入到并且更新队列 seedlist。重复以上步骤直到处理完所有的点。

- 核心距离
- 可达距离

[OPTICS: Ordering Points To Identify the Clustering Structure](http://www.crest.iu.edu/~chemuell/projects/presentations/optics-v1.pdf)
[Clustering：Model-Based Algorithm](http://isilic.iteye.com/blog/1829011)

---

#### 基于密度峰值和距离的聚类算法

核心思想在于对聚类中心的刻画上，作者认为聚类中心同时具有以下两个特点：

- 本身的密度大，即它被密度均不超过它的邻居包围（局部密度$\rho_i$）
- 与其它密度更大的数据点之间的“距离相对更大”（距离$\delta_i$）

$x_i$的局部密度$\rho_i$定义为：

$$
\rho_i = \sum_j \chi(d_{ij} - d_c)
$$

其中，

$$
\begin{equation} \chi(x)=\begin{cases} 1,& \text{if \(x<0\)};\\ 0,& otherwise. \end{cases} \end{equation}
$$

距离$\delta_i$度量$x_i$和比其密度高的最近样本点之间的距离。如果$\rho_i$为最大值，则$\delta_i$为离$x_i$最远样本之间的距离。

$$\begin{equation} \delta_i=\begin{cases} \underset{j:\rho_j>\rho_i}{\min}(d_{ij}), & \text{if \(\exists j,\rho_j>\rho_i\)};\\ \underset{j}{\max}(d_{ij}), & otherwise. \end{cases} \end{equation}$$

![][4]

http://blog.csdn.net/itplus/article/details/38926837
http://www.cnblogs.com/taokongcn/p/4034348.html
http://www.cnblogs.com/jeromeblog/p/4141902.html

---

[1]:http://7xjbdi.com1.z0.glb.clouddn.com/kmeans_2.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/hierarchical-clustering-agnes-diana.png
[3]:http://7xjbdi.com1.z0.glb.clouddn.com/2000px-DBSCAN-Illustration.svg.png
[4]:http://7xjbdi.com1.z0.glb.clouddn.com/delta_density_peak.png

[^1]: [聚类方法](https://www.zybuluo.com/frank-shaw/note/117235)
[^2]: [聚类算法初探（五）DBSCAN](http://blog.csdn.net/itplus/article/details/10088625)
