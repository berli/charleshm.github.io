---
published: true
author: Charles
layout: post
title:  "聚类算法总结"
date:   2016-06-20 14:28
categories: 机器学习
---

来公司实习已经一个多月了，做的事情主要根据聚类相关，趁着有空，来一发总结。

![整体框图][5]

> 主要总结下各类算法的思想，以及优缺点，不要太在意细节。

---

### 基于划分

#### K-means

**基本思想**： K-means算法首先随机地选择个对象，每个对象初始地代表一个簇的平均值或中心。对剩余的每个对象根据其与各个簇中心的距离（之前的各种距离的定义，在这里派上了用场，可以根据实际需求来选择不同的距离定义），将它赋给最近的簇。然后重新计算每个簇的平均值。这个过程不断重复，直到准则函数函数收敛（准则函数常常使用最小均方误差）。

---

假定输入样本为 $S = x_1,x_2,\cdots,x_m$

- 选定初始的k个类别中心，$\mu_1,\mu_2,\cdots,\mu_k$
- 对于每个样本，将其标记为距离类别中心最近的类别:

$$\text{label}_i = \underset{1 \le j \le k}{\arg \min}||x_i-\mu_j||$$

- 将每个类别中心更新为隶属于该类别的所有样本的均值

$$\mu_j = \frac{1}{|c_j|}\sum_{i \in C_j} x_i$$

- 重复最后两步，直到类别中心的变化小于某阈值

---

![][1]

#### K-means 和 EM

K-means也被称为hard EM（GMM为soft EM），整体思想与EM是相通的。

> 我们一开始不知道每个样例$x^{(i)}$对应隐含变量也就是最佳类别$c^{(i)}$。最开始可以随便指定一个$c^{(i)}$给它，然后为了让$P(x,y)$最大（这里是要让$J$最小），我们求出在给定$c$情况下，$J$最小时的$\mu$（前面提到的其他未知参数），然而此时发现，可以有更好的$c^{(i)}$（质心与样例$x^{(i)}$距离最小的类别）指定给样例$x^{(i)}$，那么$c^{(i)}$得到重新调整，上述过程就开始重复了，直到没有更好的$c^{(i)}$指定。这样从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量$c^{(i)}$，M步更新其他参数$\mu$来使$J$最小化。

---

[1]:http://7xjbdi.com1.z0.glb.clouddn.com/kmeans_2.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/hierarchical-clustering-agnes-diana.png
[3]:http://7xjbdi.com1.z0.glb.clouddn.com/2000px-DBSCAN-Illustration.svg.png
[4]:http://7xjbdi.com1.z0.glb.clouddn.com/delta_density_peak.png
[5]:http://7xjbdi.com1.z0.glb.clouddn.com/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.png

[^1]: [聚类方法](https://www.zybuluo.com/frank-shaw/note/117235)
[^2]: [聚类算法初探（五）DBSCAN](http://blog.csdn.net/itplus/article/details/10088625)
