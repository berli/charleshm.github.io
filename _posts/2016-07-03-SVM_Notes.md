---
published: true
author: Charles
layout: post
title:  "支持向量机笔记"
date:   2016-07-03 7:30
categories: 机器学习 
---

# 支持向量机笔记

标签（空格分隔）： Big-Data

---

#### 模型来源

之前我们介绍过PLA算法，对于线性可分的情况，找到一条线，能够区分开正负样本。但是PLA的解具有随机性的特点，我们会得到多组不同的解，那么哪一个才是最好的呢？

![][1]

我们可以从鲁棒性的角度来考虑，一个好的模型应该对噪声具有很强的容忍性。最右边的线离与它最近的点是最远的，能够容忍较大的测量误差，从鲁棒性的角度来讲是最佳的。

![][2]

形象点来描述就是我们要找出一条最胖的线，也就是Margin最大，

![][3]


----------


#### 数学表示

接下来，我们考虑下如何表示这个Margin，实际上就是求点到超平面的距离

> 点到平面的距离等价于点与平面内任意一点所组成的向量在平面法向量上的投影。

![][4]

原来的问题可以表示为：

![][5]

我们来做一点简化，因为对超平面进行缩放实际上对我们最后的结果不会有任何影响，我们可以做如下变换

![][6]

我们在解不变的情况下，放松下约束条件

![][7]


----------

#### 模型求解

![][8]

问题可以转化为标准的二次规划问题，

![][9]


----------


#### 对偶变换

但是直接这样解算法的是与样本的维度相关的，当样本维度很高甚至接近无穷时，这样来解就不太实际了。

![][10]

我们先利用拉格朗日乘数法将问题转化为无约束问题，

![][12]

我们可以做对偶转换，

![][13]

在对偶问题下，求解算法的复杂度与样本数量（等于拉格朗日算子a的数量）有关。

![][11]


----------

#### 对偶求解

利用偏导等于0进行化简，

![][14]

化简得，

![][15]

整理可得，

![][16]


[1]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_060536.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_061310.png
[3]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_062256.png
[4]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_063720.png
[5]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_063930.png
[6]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_065151.png
[7]:http://7xjbdi.com1.z0.glb.clouddn.com/compressed-4fz6.png
[8]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_072606.png
[9]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_073254.png
[10]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_075103.png
[11]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_075420.png
[12]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_080258.png
[13]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_081257.png
[14]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_082803.png
[15]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_083209.png
[16]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-16_083519.png