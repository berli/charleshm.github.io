---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，每个新模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

#### 算法流程


输入：训练集$\{(x_i, y_i)\}_{i=1}^n$，可微的损失函数$ L(y, F(x))$，迭代次数$M$。

算法：
\begin{enumerate}
  \item 初始化为一个常数的模型：
  $$F_0(x) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma)$$
  \item For $m = 1$ to $M$:
  \begin{enumerate}
    \item 计算伪残差（pseudo-residuals）：
    $$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)} \quad \mbox{for } i=1,\ldots,n.$$
    \item 基于pseudo-residuals训练一个基分类器。也就是用训练集$\{(x_i, y_i)\}_{i=1}^n$训练一个基分类器.
    \item 计算这个基分类器对应的权重$\gamma_m$.
    $$\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\right).$$
    \item 更新整个模型：
    $$F_m(x) = F_{m-1}(x) + \gamma_m h_m(x).$$
  \end{enumerate}
\end{enumerate}

---

#### XGBoost

GBDT+LR:

[](http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf)

推荐系统：
http://km.oa.com/group/19658/articles/show/209927?kmref=search&from_page=1&no=4&is_from_iso=1

[^1]:[决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)
[^2]:[深入浅出ML之Boosting家族](http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/)
[^3]:[A Gentle Introduction to Gradient Boosting](http://www.chengli.io/tutorials/gradient_boosting.pdf)
[^4]:[Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

