---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

##### 算法流程

假设我们的训练集为${(x_1,y_1),…,(x_n,y_n)}$，我们希望找到一个函数$F^*$，使得损失函数$L(y,F(x))$期望最小：

$$F^* = \underset{F}{\arg\min} \, \mathbb{E}_{x,y}[L(y, F(x))].{}$$

Gradient Boosting假定有着真实的$y$，我们现在需要找到由基函数$h_i(x)$加权求和组成的弱分类器来近似$ \hat{F}(x) $：

$$F(x) = \sum_{i=1}^M \gamma_i h_i(x) + \mbox{const}$$

根据经验风险最小化原则，Gradient Boosting每一步求出的函数$\hat{F}(x)$需要最小化训练集上的损失函数。我们迭代地构造这个模型，初始化$F_0(x)$为一个常数函数。

$$F_0(x) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma)$$

$$F_m(x) = F_{m-1}(x) + \underset{f \in \mathcal{H}}{\operatorname{arg\,min}} \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + f(x_i)),$$

其中$f$限定为基函数类$\mathcal{H}$中选取的函数。

然而，要找到对损失函数$L$最优的$f$是非常困难的，我们可以利用最速下降法来求解。

如果我们只关注预测训练集中的数据，$f$没有限制的话。我们可以将$L(y,f)$看成一个关于$f(x_1),...,f(x_n)$的函数，而不是关于$f$的函数，则：

$$F_m(x) = F_{m-1}(x) - \gamma_m \sum_{i=1}^n \nabla_f L(y_i, F_{m-1}(x_i)),$$

$$\gamma_m = \underset{\gamma}{\arg\min} \sum_{i=1}^n L\left(y_i, F_{m-1}(x_i) -\gamma \frac{\partial L(y_i,F_{m-1}(x_i))}{\partial f(x_i)} \right)$$

这样就求出了$f$，不过f如果必须要从某一个限定类别的函数类（决策树）中选取，我们就选取类中最接近它的梯度的$f$。乘子$\gamma$可以通过 line search 求出来。


> $$\{ \\   \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\   \qquad\quad\; 损失函数L(y, f(x)); \\   \quad 输出：提升树\hat{f}(x). \\   \quad 过程: \\   \qquad (1). 初始化模型 \\   \qquad\qquad\qquad f_0(x) = \arg \min_c \sum_{i=1}^{M} L(y^{(i)}, c)； \\   \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\   \qquad\qquad (a). 计算残差：对于i=1,2,\cdots,M \\   \qquad\qquad\qquad\qquad r_{ki} = -\left[ \frac{\partial L(y^{(i)}, \; f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \\   \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到第k颗树的叶结点区域R_{kj}，\quad j=1,2,\cdots,J \\   \qquad\qquad (c). 对j=1,2,\cdots,J, 计算：\\   \qquad\qquad\qquad\qquad c_{kj} = \arg \min_c \sum_{x^{(i)} \in R_{kj}} L(y^{(i)}, \; f_{k-1}(x^{(i)}) + c)\\   \qquad\qquad (d). 更新模型：\\   \qquad\qquad\qquad\qquad    f_k(x) = f_{k-1}(x) + \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\   \qquad\; (3). 得到回归提升树 \\   \qquad\qquad\qquad \hat{f}(x) = f_K(x) = \sum_{k=1}^{K} \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\    \}$$
