---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，每个新模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

#### 算法流程



$$
\begin{align}     
&\{ \\     &\quad 输入：训练集\{(x_i, y_i)\}_{i=1}^n，可微的损失函数 L(y, F(x))，迭代次数M，\\     
&\qquad\qquad y^{(i)} \in \mathcal{Y} = \{-1, +1\}, 弱分类器; \\     
&\quad 输出：最终分类器G(x). \\     
&\quad 过程：\\     
&\qquad (1). 初始化训练数据的权值分布 \\     
&\qquad\qquad\qquad D_1=(w_{11}, w_{12}, \cdots, w_{1M}), \quad w_{1i}=\frac{1}{M}, \; i=1,2,\cdots,M \\     
&\qquad (2). 训练K个弱分类器 k=1,2,\cdots,K \\     &\qquad\qquad (a). 使用具有权值分布D_k的训练数据集学习，得到基本分类器 \\     
&\qquad\qquad\qquad\qquad G_k(x): \mathcal{X} \rightarrow \{-1, +1\} \qquad\qquad(ml.1.6.3)\\     
&\qquad\qquad (b). 计算G_k(x)在训练数据集上的分类误差率 \\     
&\qquad\qquad\qquad\qquad e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{i=1}^{M} w_{ki} I(G_k(x^{(i)}) \not= y^{(i)}) \qquad(ml.1.6.4)\\     
&\qquad\qquad (c). 计算G_k(x)的系数 \\     
&\qquad\qquad\qquad\qquad \alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k} \quad(e是自然对数) \qquad(ml.1.6.5)\\     
&\qquad\qquad (d). 更新训练数据集的权值分布 \\     &\qquad\qquad\qquad\qquad D_{k+1} = (w_{k+1,1}, w_{k+1,2}, \cdots, w_{k+1,M})\\     
&\qquad\qquad\qquad\qquad w_{k+1,i} = \frac{w_{k,i}}{Z_k} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})), \quad i=1,2,\cdots,M \quad(ml.1.6.6)\\     
&\qquad\qquad\qquad Z_k是规范化因子 \\     
&\qquad\qquad\qquad\qquad Z_k = \sum_{i=1}^{M} w_{k,i} \cdot \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \qquad(ml.1.6.7)\\     
&\qquad\qquad\qquad 使D_{k+1}成为一个概率分布。\\     
&\qquad (3). 构建基本分类器的线性组合 \\     
&\qquad\qquad\qquad f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \qquad(ml.1.6.8)\\     
&\qquad 得到最终的分类器 \\     
&\qquad\qquad G(x) = sign (f(x)) = sign \left(\sum_{k=1}^{K} \alpha_k G_k(x) \right) \qquad(ml.1.6.9)\\     
&\}
$$

输入：训练集$\{(x_i, y_i)\}_{i=1}^n$，可微的损失函数$ L(y, F(x))$，迭代次数$M$。

算法：
\begin{enumerate}
  \item 初始化为一个常数的模型：
  $$F_0(x) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma)$$
  \item For $m = 1$ to $M$:
  \begin{enumerate}
    \item 计算伪残差（pseudo-residuals）：
    $$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)} \quad \mbox{for } i=1,\ldots,n.$$
    \item 基于pseudo-residuals训练一个基分类器。也就是用训练集$\{(x_i, y_i)\}_{i=1}^n$训练一个基分类器.
    \item 计算这个基分类器对应的权重$\gamma_m$.
    $$\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\right).$$
    \item 更新整个模型：
    $$F_m(x) = F_{m-1}(x) + \gamma_m h_m(x).$$
  \end{enumerate}
\end{enumerate}

---

#### XGBoost


[^1]:[决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)
[^2]:[深入浅出ML之Boosting家族](http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/)

