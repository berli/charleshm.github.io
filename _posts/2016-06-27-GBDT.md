---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，每个新模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

#### 算法流程

$\{ \\   \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\   \qquad\quad\; 损失函数L(y, f(x)); \\   \quad 输出：提升树\hat{f}(x). \\   \quad 过程: \\   \qquad (1). 初始化模型 \\   \qquad\qquad\qquad f_0(x) = \arg \min_c \sum_{i=1}^{M} L(y^{(i)}, c)； \\   \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\   \qquad\qquad (a). 计算残差：对于i=1,2,\cdots,M \\   \qquad\qquad\qquad\qquad r_{ki} = -\left[ \frac{\partial L(y^{(i)}, \; f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \\   \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到第k颗树的叶结点区域R_{kj}，\quad j=1,2,\cdots,J \\   \qquad\qquad (c). 对j=1,2,\cdots,J, 计算：\\   \qquad\qquad\qquad\qquad c_{kj} = \arg \min_c \sum_{x^{(i)} \in R_{kj}} L(y^{(i)}, \; f_{k-1}(x^{(i)}) + c)\\   \qquad\qquad (d). 更新模型：\\   \qquad\qquad\qquad\qquad    f_k(x) = f_{k-1}(x) + \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\   \qquad\; (3). 得到回归提升树 \\   \qquad\qquad\qquad \hat{f}(x) = f_K(x) = \sum_{k=1}^{K} \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\    \}$


