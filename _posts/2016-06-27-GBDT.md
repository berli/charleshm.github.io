---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，每个新模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

#### 算法流程

[Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

---

#### XGBoost

GBDT+LR:

[](http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf)

推荐系统：
http://km.oa.com/group/19658/articles/show/209927?kmref=search&from_page=1&no=4&is_from_iso=1

---

#### XGBoost vs GBDT

> 以下内容主要摘自 [wepon](http://2hwp.com/) 博客。

- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

![][1]

- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

![][2]

---

#### 新进展

[Support Dropout on XGBoost](http://dmlc.ml/xgboost/2016/07/02/support-dropout-on-xgboost.html)

[1]:http://7xjbdi.com1.z0.glb.clouddn.com/xgb%2BTaylor.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/xgb_c_t.png


[^1]:[决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)
[^2]:[深入浅出ML之Boosting家族](http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/)
[^3]:[A Gentle Introduction to Gradient Boosting](http://www.chengli.io/tutorials/gradient_boosting.pdf)
[^4]:[Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

