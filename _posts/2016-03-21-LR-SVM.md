---
published: true
author: Charles
layout: post
title:  "Logistic Regression and SVM"
date:   2016-03-21 9:30
categories: 机器学习
---

逻辑回归和支持向量机之间的区别也是面试经常会问的一道题，特地找了一些相关资料看了下。

#### 损失函数
我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数[^1]：

$$\begin{align*}
\text{SVM} : & \frac{1}{n}\sum_{i=1}^n(1-y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1])^+ + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{1}\\
\text{Logistic} : & \frac{1}{n}\sum_{i=1}^n \overbrace{-\log g(y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1])}^{-\log P(y_i|\mathbf{x},\mathbf{W})} + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{2}\\
\end{align*}$$

其中， $g(z)=(1+\exp(-z))^{-1}$.

可以将两者统一起来，

$$\begin{align*}
\text{Both}: & \frac{1}{n}\sum_{i=1}^n Loss(\overbrace{y_i[w_0+\mathbf{x}_i^T\mathbf{w}_1]}^{z}) + \lambda \lVert \mathbf{w}_1 \rVert/2 \tag{3}
\end{align*}$$


> 区别在于逻辑回归采用的是 log loss（对数损失函数），svm采用的是hinge loss $\rightarrow E(z) = \max(0,1-z)$。

 - SVM: $Loss(z) = (1-z)^+$
 - LR: $Loss(z) = \log(1+\exp(-z))$

![此处输入图片的描述][1]

----------

这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。svm考虑局部（支持向量），而logistic回归考虑全局，就像大学里的辅导员和教师间的区别[^2]。
 

> 辅导员关心的是挂科边缘的人，常常找他们谈话，告诫他们一定得好好学习，不要浪费大好青春，挂科了会拿不到毕业证、学位证等等，相反，对于那些相对优秀或者良好的学生，他们却很少去问，因为辅导员相信他们一定会按部就班的做好分内的事；而大学里的教师却不是这样的，他们关心的是班里的整体情况，大家是不是基本都理解了，平均分怎么样，至于某个人的分数是59还是61，他们倒不是很在意。


----------


  [1]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-22_195233.png?imageView2/2/w/400
  
  [^1]: [Machine learning: lecture 7](http://www.ai.mit.edu/courses/6.867-f04/lectures/lecture-7-ho.pdf)
  [^2]: [SVM(一) 基本概念、模型建立](http://zhihaozhang.github.io/2014/05/08/svm1/)