---
published: true
author: Charles
layout: post
title:  "机器学习技法第十课笔记"
date:   2016-07-07 7:30
categories: 机器学习 
---

![][1]


----------


#### 随机森林

前面我们已经知道Bagging可以降低模型的variance，而决策树（特别是完全生长的决策树）bias较低，variance较高。

![][2]


----------


Random Forest可以看做利用Bagging的思想对多个决策树模型做融合。

![][3]


----------


为了使得每次生成的决策树**模型**尽可能不同，一方面我们通过Bootstrapping抽取不同的训练集，另一方面我们在决策树每一次分裂的时候，对特征进行列采样。

![][4]


----------

![][5]


----------


#### OOB Estimation

每次利用Bootstrapping抽取样本的时候，因为是有放回的抽样，会有一部分样本不会被抽到。

![][6]


----------


假设总共N份样本，每次训练决策树模型我们有放回的抽取N个样本，大概会有1/3的样本抽不到。

![][7]


----------

袋外误差估计，

![][8]


----------

#### Feature Selection

一种直观的思路是我们能不能想办法计算每个特征的重要性？

![][9]


----------


比如在线性模型里面，我们可以用训练出来的权重来衡量特征的重要性。

![][10]


----------

> 基本思想： 如果某维特征很重要，当我们随机改变特征的取值（不改变原来分布的情况下），我们的结果将会受到很大的影响。

![][11]


----------

那么这样的话，在随机改变特征训练模型，有没有啥偷懒的办法呢？

> 我们可以在验证的时候再对特征做permutation，而不是在训练的时候，这样我们就不用重新训练模型。

![][12]


----------

总结一下，

![][13]


----------

随机数的设置会一定程度影响我们的结果。

![][14]

[1]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_115517.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_120327.png
[3]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_120830.png
[4]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_121612.png
[5]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_121733.png
[6]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_122356.png
[7]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_122905.png
[8]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_123139.png
[9]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_124230.png
[10]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_124501.png
[11]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_125109.png
[12]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_130143.png
[13]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_130742.png
[14]:http://7xjbdi.com1.z0.glb.clouddn.com/2016-09-24_133930.png